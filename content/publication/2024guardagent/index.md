---
title: "GuardAgent: Safeguard LLM Agents by a Guard Agent via Knowledge-Enabled Reasoning"

# Authors
# If you created a profile for a user (e.g. the default `admin` user), write the username (folder name) here 
# and it will be replaced with their full name and linked to their profile.
authors:
- Zhen Xiang
- Linzhi Zheng
- Yanjie Li
- admin
- Qinbin Li
- Han Xie
- Jiawei Zhang
- Zidi Xiong
- Chulin Xie
- Carl Yang
- Dawn Song
- Bo Li


date: "2025-04-30T13:08:20+08:00"
doi: ""

# Schedule page publish date (NOT publication's date).
# publishDate: "2017-01-01T00:00:00Z"
publishDate: "2025-04-30T13:08:20+08:00"

# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types: ["3"]

# Publication name and optional abbreviated publication name.
publication: In the *International Conference on Machine Learning*
publication_short: ICML

abstract: "The rapid advancement of large language models (LLMs) has catalyzed the deployment of LLM-powered agents across numerous applications, raising new concerns regarding their safety and trustworthiness. Existing methods for enhancing the safety of LLMs are not directly transferable to LLM-powered agents due to their diverse objectives and output modalities. In this paper, we propose GuardAgent, the first LLM agent as a guardrail to other LLM agents. Specifically, GuardAgent oversees a target LLM agent by checking whether its inputs/outputs satisfy a set of given guard requests defined by the users. GuardAgent comprises two steps: 1) creating a task plan by analyzing the provided guard requests, and 2) generating guardrail code based on the task plan and executing the code by calling APIs or using external engines. In both steps, an LLM is utilized as the core reasoning component, supplemented by in-context demonstrations retrieved from a memory module. Such knowledge-enabled reasoning allows GuardAgent to understand various textual guard requests and accurately translate them into executable code that provides reliable guardrails. Furthermore, GuardAgent is equipped with an extendable toolbox containing functions and APIs and requires no additional LLM training, which underscores its generalization capabilities and low operational overhead. Additionally, we propose two novel benchmarks: an EICU-AC benchmark for assessing privacy-related access control for healthcare agents and a Mind2Web-SC benchmark for safety evaluation for web agents. We show the effectiveness of GuardAgent on these two benchmarks with 98.7% and 90.0% accuracy in moderating invalid inputs and outputs for the two types of agents, respectively. We also show that GuardAgent is able to define novel functions in adaption to emergent LLM agents and guard requests, which underscores its strong generalization capabilities."

# Summary. An optional shortened abstract.
summary: The first automated guardrail for agents.

tags: ["Selected", "LLM", "Large Models", "AI Safety", "Trustworthy"]

# Display this page in the Featured widget?
featured: false

# Custom links (uncomment lines below)
# links:
# - name: Custom Link
#   url: http://example.org

url_pdf: 'https://arxiv.org/abs/2406.09187'
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''
# url_custom:
links:
  - name: "üèÅ Competition"
    url: "https://www.llmagentsafetycomp24.com/"

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
# image:
#   caption: 'Outsourcing Training without Uploading Data'
#   focal_point: "center"
#   preview_only: true

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `internal-project` references `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects:
  - "holistic-trustworthy"

# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides: "example"` references `content/slides/example/index.md`.
#   Otherwise, set `slides: ""`.
slides: ""  # example

math: true
---


**Disclaim**: The blog is automatically generated by AI and could contain misinformation.

## GuardAgent: A New Guardrail for LLM Agents

The rapid rise of large language model (LLM) agents has brought new safety and security challenges, especially as these agents are deployed in sensitive domains like healthcare and web automation. Traditional guardrails for LLMs focus on moderating text, but LLM agents require more flexible and reliable safeguards due to their diverse actions and outputs.

### What is GuardAgent?
GuardAgent is the first LLM agent designed to act as a guardrail for other LLM agents. It dynamically checks whether a target agent's actions comply with user-defined safety requests. GuardAgent works in two main steps:
1. **Task Planning**: Analyzes safety guard requests and generates a step-by-step plan using an LLM, enhanced by examples from a memory module.
2. **Guardrail Code Generation**: Translates the plan into executable code, which is run to enforce the guard requests. The toolbox of GuardAgent can be extended with new functions and APIs as needed.

This approach enables GuardAgent to flexibly adapt to new agents and safety requirements, providing reliable, code-based guardrails without retraining the underlying LLMs.

### Key Features
- **Knowledge-Enabled Reasoning**: Uses in-context learning and memory retrieval to understand and enforce complex safety requests.
- **Extendable Toolbox**: Users can upload new functions or APIs to handle novel guard requests.
- **Non-Invasive**: GuardAgent operates alongside the target agent, ensuring safety without degrading the agent's original performance.
- **No Extra Training Needed**: Works with off-the-shelf LLMs, reducing operational overhead.

---

![GuardAgent Framework Overview](https://arxiv.org/html/2406.09187v3/extracted/6491580/figures/figure1_v5.png)
*Figure: GuardAgent safeguards target agents by analyzing safety requests, planning, and generating guardrail code for enforcement.*

---

### Benchmarks and Results
GuardAgent introduces two new benchmarks:
- **EICU-AC**: Evaluates privacy-related access control for healthcare agents.
- **Mind2Web-SC**: Assesses safety policy enforcement for web agents.

On these benchmarks, GuardAgent achieves impressive results:
- **98.7% accuracy** in moderating invalid inputs/outputs for healthcare agents
- **90.0% accuracy** for web agents
- Outperforms both hardcoded and model-based guardrails, especially in complex scenarios

**Performance Table:**

| Core LLM | Method                | EICU-AC LPA | Mind2Web-SC LPA |
|----------|----------------------|-------------|-----------------|
| GPT-4    | GuardAgent           | 98.7%       | 90.0%           |
| GPT-4    | Model-Guarding-Agent | 97.5%       | 82.5%           |
| GPT-4    | Hardcoded Rules      | 81.0%       | 77.5%           |
| Llama3   | GuardAgent           | 98.4%       | 84.5%           |

*Table: GuardAgent outperforms baselines on both benchmarks (LPA = Label Prediction Accuracy).*

---

![Case Study: GuardAgent vs Baseline](https://arxiv.org/html/2406.09187v3/extracted/6491580/figures/guardagent_case_study.png)
*Figure: GuardAgent strictly enforces access control, avoiding mistakes made by model-based baselines.*

---

### Why Does GuardAgent Work?
Unlike hardcoded rules or simple prompt-based moderation, GuardAgent leverages code generation and execution, making it robust to ambiguous or complex safety requirements. Its memory module and extendable toolbox allow it to generalize to new tasks and agents, while its non-invasive design ensures that the original agent's utility is preserved.

---

![Breakdown of GuardAgent Results](https://arxiv.org/html/2406.09187v3/extracted/6491580/figures/spider1.png)
*Figure: GuardAgent achieves high accuracy across all roles and rules in both benchmarks.*

---

### Real-World Impact
GuardAgent represents a significant step toward trustworthy and safe deployment of LLM agents in real-world applications. Its flexible, code-based approach can be adapted to a wide range of domains, from healthcare privacy to web automation safety.

**Learn more:** [arXiv paper](https://arxiv.org/abs/2406.09187) | [Competition](https://www.llmagentsafetycomp24.com/) | [Project page](https://guardagent.github.io/)

