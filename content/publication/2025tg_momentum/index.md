---
title: "Scaling Textual Gradients via Sampling-Based Momentum"

authors:
- Zixin Ding
- Junyuan Hong
- Jiachen T. Wang
- Zinan Lin
- Zhangyang Wang
- Yuxin Chen

date: "2025-05-31T00:00:00Z"
doi: ""

publishDate: "2025-05-31T00:00:00Z"

publication_types: ["3"]

publication: "ArXiv"
publication_short: "ArXiv"

abstract: "As prompts play an increasingly critical role in large language models (LLMs), optimizing textual prompts has become a crucial challenge. The Textual Gradient Descent (TGD) framework has emerged as a promising data-driven approach that iteratively refines textual prompts using LLM-suggested updates (or textual gradients) over minibatches of training samples. In this paper, we empirically demonstrate that scaling the number of training examples initially improves but later degrades TGD's performance across multiple downstream NLP tasks. However, while data scaling improves results for most tasks, it also significantly increases the computational cost when leveraging LLMs. To address this, we draw inspiration from numerical gradient descent and propose Textual Stochastic Gradient Descent with Momentum (TSGD-M) - a method that facilitates scalable in-context learning by reweighting prompt sampling based on past batch distributions. Across nine NLP tasks spanning three domains - including BIG-Bench Hard (BBH), natural language understanding tasks, and reasoning tasks - TSGD-M significantly outperforms TGD baselines that do not incorporate reweighted sampling, while also reducing variance in most tasks."

summary: "A momentum-based, sampling-driven method for scaling textual gradient optimization in LLM prompt engineering, improving performance and efficiency across diverse NLP tasks."

tags: ["LLM", "Prompt Optimization", "Gradient Descent", "Sampling", "Momentum", "NLP"]

featured: false

url_pdf: 'https://arxiv.org/abs/2506.00400'
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''

projects:
  - ""

slides: ""

math: true
---

**Disclaim**: The blog is automatically generated by AI and could contain misinformation.

## Key Innovation: Scaling Textual Gradients with Sampling-Based Momentum

Prompt optimization is a key challenge for LLMs. This work introduces **Textual Stochastic Gradient Descent with Momentum (TSGD-M)**, a scalable, sampling-based method for prompt optimization that leverages momentum to improve both performance and efficiency.

### The Prompt Optimization Problem

- **Textual Gradient Descent (TGD)** iteratively refines prompts using LLM feedback, but scaling up data can eventually degrade performance and increase computational cost.
- **Prompt sampling and reweighting** are critical for efficient and robust optimization.

### TSGD-M: Sampling-Based Momentum Method

- **Inspired by numerical gradient descent with momentum**: TSGD-M reweights prompt sampling based on past batch distributions, facilitating scalable in-context learning.
- **Reduces variance**: Achieves more stable and robust prompt optimization across tasks.
- **Generalizable**: Effective across nine NLP tasks in three domains, including BIG-Bench Hard (BBH), NLU, and reasoning tasks.

### Experimental Results

- **Outperforms TGD baselines**: TSGD-M achieves higher performance and lower variance than TGD without reweighted sampling.
- **Efficient scaling**: Maintains or improves results while reducing computational cost.

### Why TSGD-M Works

- **Momentum and sampling**: By incorporating momentum and adaptive sampling, TSGD-M avoids overfitting and instability seen in vanilla TGD at scale.
- **Applicable to many tasks**: Demonstrated effectiveness across a wide range of NLP benchmarks.

### Conclusion

TSGD-M is a practical, scalable solution for prompt optimization in LLMs, leveraging sampling-based momentum to improve both performance and efficiency.

**Paper Available**: [arXiv:2506.00400](https://arxiv.org/abs/2506.00400)
