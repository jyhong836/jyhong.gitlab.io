---
title: "SEAL: Steerable Reasoning Calibration of Large Language Models for Free"

# Authors
# If you created a profile for a user (e.g. the default `admin` user), write the username (folder name) here 
# and it will be replaced with their full name and linked to their profile.
authors:
- Runjin Chen
- Zhenyu Zhang
- admin
- Souvik Kundu
- Zhangyang Wang


date: "2025-04-07T00:00:00Z"
doi: ""

# Schedule page publish date (NOT publication's date).
# publishDate: "2017-01-01T00:00:00Z"
publishDate: "2025-05-06T00:00:00Z"

# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types: ["3"]

# Publication name and optional abbreviated publication name.
publication: "ArXiv"
publication_short: "ArXiv"

abstract: "Large Language Models (LLMs), such as OpenAI's o1-series have demonstrated compelling capabilities for complex reasoning tasks via the extended chain-of-thought (CoT) reasoning mechanism. However, recent studies reveal substantial redundancy in the CoT reasoning traces, which not only increases inference latency but also negatively impacts model performance by diverting attention to unnecessary reasoning paths. To address this issue, we investigate the internal reasoning structures of LLMs and categorize them into three primary thought types: execution, reflection, and transition thoughts. Moreover, our analysis reveals that excessive reflection and transition thoughts are strongly correlated with failure cases and these thought categories exhibit clear separation in the latent space. Based on these, we introduce SEAL (Steerable reasoning calibration), a training-free approach that seamlessly calibrates the CoT process, improving accuracy while demonstrating significant efficiency gains. SEAL consists of an offline stage for extracting the reasoning steering vector in the latent space, followed by an on-the-fly calibration of the reasoning trace through representation intervention using the steering vector. Notably, the steering vector exhibits strong transferability across various tasks. Extensive experiments across multiple models (DeepSeek-R1-Distill and QwQ-32B-Preview) and benchmarks (Math500, GSM8K, LiveCodeBench) validate the effectiveness of SEAL, up to a 11% improvement in accuracy while reducing reasoning tokens by 11.8% to 50.4%."

# Summary. An optional shortened abstract.
summary: A training-free approach that calibrates chain-of-thought reasoning in LLMs, improving accuracy while reducing computational overhead.

tags: ["LLM", "Large Models", "Chain-of-Thought", "Reasoning", "Efficiency", "CoT"]

# Display this page in the Featured widget?
featured: false

# Custom links (uncomment lines below)
# links:
# - name: Custom Link
#   url: http://example.org

url_pdf: 'https://arxiv.org/abs/2504.07986'
url_code: 'https://github.com/VITA-Group/SEAL'
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''
# url_custom:
# links:

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
# image:
#   caption: 'SEAL: Steerable Reasoning Calibration'
#   focal_point: "center"
#   preview_only: true

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `internal-project` references `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects:
  - "holistic-trustworthy"

# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides: "example"` references `content/slides/example/index.md`.
#   Otherwise, set `slides: ""`.
slides: ""  # example

math: true
---

**Disclaim**: The blog is automatically generated by AI and could contain misinformation. 

## Key Innovation: Making LLM Reasoning More Efficient

Large Language Models like OpenAI's o1-series have shown impressive reasoning capabilities through extended Chain-of-Thought (CoT) mechanisms. However, our research reveals a critical inefficiency: **substantial redundancy in reasoning traces** that hurts both performance and efficiency.

![SEAL Framework Overview](https://arxiv.org/html/2504.07986v2/x4.png)
*Figure: Overview of our SEAL framework showing offline extraction and online intervention stages*

### The Reasoning Redundancy Problem

We discovered that current CoT reasoning suffers from significant issues:
- üêå **Increased inference latency** due to unnecessary reasoning steps
- ‚ùå **Degraded performance** from attention being diverted to irrelevant paths
- üí∏ **Higher computational costs** from processing redundant tokens

Recent studies show that LLMs often determine the correct final answer early in the reasoning process but continue generating excessive and redundant thought sequences. This inefficient reasoning can even degrade final performance as models become trapped in redundant verification loops.

### Understanding Reasoning Structure

Our systematic analysis categorizes LLM internal reasoning into three distinct thought types:

![Reasoning Patterns Example](https://arxiv.org/html/2504.07986v2/x1.png)
*Figure: Example showing decomposition of reasoning into different thought types*

1. **Execution Thoughts**: Core problem-solving steps where the model analyzes and solves problems step by step
2. **Reflection Thoughts**: Self-evaluation and verification where the model pauses to verify its steps
3. **Transition Thoughts**: Paradigm shifts where the model rethinks problems from different perspectives

### Statistical Evidence of Redundancy

![Thought Statistics](https://arxiv.org/html/2504.07986v2/x2.png)
*Figure: Statistics showing thought distribution in correct vs incorrect samples*

**Key Findings from Our Analysis**:
- For samples of the same difficulty level, **incorrect samples contain significantly more thoughts** than correct ones
- The increase is largely driven by **excessive reflection and transition thoughts**
- Each reflection/transition step typically triggers several execution steps, creating cascading inefficiency
- **Stronger correlation**: Excessive reflection and transition thoughts are strongly correlated with failure cases

### Latent Space Separability

![t-SNE Visualization](https://arxiv.org/html/2504.07986v2/x3.png)
*Figure: t-SNE visualization showing clear separation of thought types in latent space*

Our latent space analysis reveals crucial insights:
- **Execution thoughts are clearly separable** from non-execution thoughts in deep layers
- **Better separability in deeper layers** - shallow layers capture low-level features while deeper layers encode conceptual knowledge
- **Reflection and transition thoughts are more similar** to each other than to execution thoughts

### SEAL: Training-Free Solution

We introduce **SEAL (Steerable Reasoning Calibration)** - a novel training-free approach that addresses these inefficiencies through a two-stage process:

#### Stage 1: Offline Extraction
- **Data Collection**: Use ~1000 training samples from reasoning benchmarks
- **Thought Categorization**: Classify thoughts using keyword identification (e.g., "Alternatively" ‚Üí transition thought)
- **Vector Computation**: Calculate reasoning steering vector as **S = HÃÑ_E - HÃÑ_RT** where:
  - HÃÑ_E = average execution thought representations
  - HÃÑ_RT = average reflection + transition thought representations

#### Stage 2: Online Intervention
- **Real-time Calibration**: Apply steering vector during inference via **HÃÉ = H + Œ±¬∑S**
- **Minimal Overhead**: Negligible computational cost compared to forward pass
- **Dynamic Adjustment**: Intervene at optimal layers (typically mid-to-late layers)

### Comprehensive Experimental Results

#### Performance Across Models and Benchmarks

**Models Tested**: DeepSeek-R1-Distill (1.5B, 7B), QwQ-32B-Preview
**Benchmarks**: Math500, GSM8K, LiveCodeBench

![Comparison Results](https://arxiv.org/html/2504.07986v2/x5.png)
*Figure: Comparison showing SEAL's superior performance over logit penalty methods*

#### Impressive Results

SEAL demonstrates significant improvements across multiple models and benchmarks:

- ‚úÖ **Up to 14.1% accuracy improvement** (Math500 hard problems)
- üöÄ **11.8% to 50.4% reduction in reasoning tokens**
- üéØ **Strong transferability** - steering vectors from Math500 work on GSM8K and LiveCodeBench
- ‚ö° **37.9% average reduction in response time** with up to 86.61% in best cases
- üìä **Consistent gains** across all tested models and tasks

#### Detailed Performance Tables

**Math500 Results:**
| Model | Method | Accuracy (%) | Tokens | Hard Accuracy (%) | Hard Tokens |
|-------|--------|--------------|---------|-------------------|-------------|
| R1-Distill-1.5B | Base | 67.0 | 4526 | 54.2 | 5737 |
| R1-Distill-1.5B | SEAL | **76.6** (+9.6) | **3340** | **63.7** (+9.5) | **4552** |
| R1-Distill-7B | Base | 85.8 | 3389 | 79.8 | 4176 |
| R1-Distill-7B | SEAL | **89.4** (+3.6) | **2661** | **84.0** (+4.2) | **3365** |

**Cross-Domain Generalization:**
| Task | Model | Base Acc | SEAL Acc | Token Reduction |
|------|-------|----------|----------|-----------------|
| GSM8K | R1-7B | 88.0% | **88.4%** (+0.4) | 28.9% |
| LiveCodeBench | R1-7B | 44.5% | **51.7%** (+7.2) | 12.9% |

### Why SEAL Outperforms Token-Level Methods

**Limitation of Logit Penalty**: Operates on individual tokens (e.g., "wait", "alternatively") rather than conceptual level

**SEAL's Advantage**: 
- Suppresses entire reflection/transition **concepts** rather than specific tokens
- Prevents models from using rephrased expressions to continue unwanted reasoning patterns
- Achieves deeper conceptual control through latent space intervention

### Ablation Studies and Analysis

#### Optimal Steering Configuration
![Steering Layer Analysis](https://arxiv.org/html/2504.07986v2/x6.png)
*Figure: Ablation study showing optimal steering layers*

- **Best Layers**: Mid-to-late layers (Layer 20 for smaller models, Layer 55 for QwQ-32B)
- **Steering Strength**: Œ± = 1.0 provides optimal balance
- **Vector Composition**: S = HÃÑ_E - HÃÑ_RT works best (weakening both reflection and transition)

#### Efficiency Analysis
![Sequence Length Comparison](https://arxiv.org/html/2504.07986v2/x8.png)
*Figure: SEAL significantly reduces sequence length for incorrect samples*

**Key Efficiency Metrics**:
- **Average reduction ratio**: 32.9-37.9% in response time
- **Maximum reduction**: Up to 86.61% for some samples
- **Throughput improvement**: ~2 tokens/second increase due to reduced KV cache overhead

### Real-World Impact Example

![Reasoning Loop Example](https://arxiv.org/html/2504.07986v2/extracted/6415071/figs/examples_app2.png)
*Figure: Example showing how excessive reflection leads to incorrect answers despite finding the correct solution multiple times*

**Case Study**: In this Math500 example, the model:
1. ‚úÖ Correctly solves the problem (answer: 12) within a few steps
2. ‚ùå Continues with excessive verification and rechecking
3. üîÑ Gets trapped in reflection loops, switching thoughts repeatedly  
4. ‚ùå Eventually deviates from correct reasoning path and produces wrong answer

**SEAL's Solution**: By reducing excessive reflection thoughts, SEAL helps models stick with their correct initial reasoning.

### Bottom Line

SEAL proves that **less can indeed be more** in LLM reasoning. By intelligently calibrating the reasoning process, we achieve better accuracy with significantly fewer computational resources, making advanced reasoning more accessible and efficient.

**Code Available**: Our implementation is publicly available on [GitHub](https://github.com/VITA-Group/SEAL), enabling researchers and practitioners to easily apply SEAL to their own models and tasks.
