---
title: "LLM-PBE: Assessing Data Privacy in Large Language Models"

# Authors
# If you created a profile for a user (e.g. the default `admin` user), write the username (folder name) here 
# and it will be replaced with their full name and linked to their profile.
authors:
- Qinbin Li
- admin
- Chulin Xie
- Jeffrey Tan
- Rachel Xin
- Junyi Hou
- Xavier Yin
- Zhun Wang
- Dan Hendrycks
- Zhangyang Wang
- Bo Li
- Bingsheng He
- Dawn Song

# Author notes (optional)
author_notes:
- "Equal contribution"
- "Equal contribution"
- "Equal contribution"

date: "2024-06-29T13:08:20+08:00"
doi: ""

# Schedule page publish date (NOT publication's date).
# publishDate: "2017-01-01T00:00:00Z"
publishDate: "2024-06-29T13:08:20+08:00"

# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types: ["1"]

# Publication name and optional abbreviated publication name.
# publication: In *ICLR 2024 Workshop on Secure and Trustworthy Large Language Models*
# publication_short: SeT@ICLR
publication: In *The 50th International Conference on Very Large Databases* (Best Paper Nomination)
publication_short: VLDB (Best Paper Finalist)

abstract: Large Language Models (LLMs) have swiftly become integral to numerous technological domains, significantly advancing applications in data management, mining, and analysis. Their profound capabilities in processing and interpreting complex language data, however, bring to light pressing concerns regarding data privacy, especially the risk of unintentional training data leakage. Despite the critical nature of this issue, there has been no existing literature to offer a comprehensive assessment of data privacy risks in LLMs. Addressing this gap, our paper introduces LLM-PBE, a toolkit crafted specifically for the systematic evaluation of data privacy risks in LLMs. LLM-PBE is designed to analyze privacy across the entire lifecycle of LLMs, incorporating diverse attack and defense strategies, and handling various data types and metrics. Through detailed experimentation with multiple LLMs, LLM-PBE facilitates an in-depth exploration of data privacy concerns, shedding light on influential factors such as model size, data characteristics, and evolving temporal dimensions. This study not only enriches the understanding of privacy issues in LLMs but also serves as a vital resource for future research in the field. Aimed at enhancing the breadth of knowledge in this area, the findings, resources, and our full technical report are made available at https://llm-pbe.github.io/ , providing an open platform for academic and practical advancements in LLM privacy assessment. 

# Summary. An optional shortened abstract.
summary: A comprehensive privacy assessment of LLMs.

tags: ["Trustworthy", "Selected", "Large Models", "Privacy"]

# Display this page in the Featured widget?
featured: false

# Custom links (uncomment lines below)
# links:
# - name: Custom Link
#   url: http://example.org

url_pdf: 'https://arxiv.org/abs/2408.12787'
url_code: 'https://github.com/QinbinLi/LLM-PBE'
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''
# url_custom:
links:
  - name: "üåç Website"
    url: "https://llm-pbe.github.io/"
  - name: "üèÅ Competition"
    url: "https://llm-pc.github.io/"
  - name: "üèÜ Best Paper Nomination"
    url: "https://llm-pbe.github.io/vldb2024_nomination_Qinbin.pdf"
  - name: "Finetune Code"
    url: "https://github.com/jyhong836/llm-dp-finetune"

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
# image:
#   caption: 'Outsourcing Training without Uploading Data'
#   focal_point: "center"
#   preview_only: true

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `internal-project` references `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects:
  - "holistic-trustworthy"

# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides: "example"` references `content/slides/example/index.md`.
#   Otherwise, set `slides: ""`.
slides: ""  # example

math: true
---

